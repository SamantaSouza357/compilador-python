"""Um analisador l√©xico que produz tokens e marca√ß√µes de indenta√ß√£o.

Reconhece identificadores, n√∫meros, strings, operadores, delimitadores, NEWLINE
e emite INDENT/DEDENT com base no espa√ßo em branco no in√≠cio de cada linha.
"""

from __future__ import annotations

import re
from typing import List

from lexer.tokens import TokenType, Token, KEYWORDS, SIMBOLOS
from lexer.errors import LexicalError


class LexerPython:
    """Converte o c√≥digo-fonte em uma lista plana de tokens, incluindo INDENT/DEDENT."""
    TOKEN_REGEX = [
        # A ordem importa: padr√µes mais espec√≠ficos primeiro
        (r"[ \t]+", None),                              # ignora espa√ßos/tabs (n√£o no in√≠cio da linha)
        (r"#.*", None),                                 # ignora coment√°rios de linha
        (r'("""(?:.|\n)*?"""|\'\'\'(?:.|\n)*?\'\'\')', None),  # ignora strings de aspas triplas (docstrings / coment√°rios de bloco)
        (r"\"[^\"]*\"|\'[^\']*\'", TokenType.STRING),   # strings normais
        (r"[0-9]+(\.[0-9]+)?", TokenType.NUMBER),       # n√∫meros
        (r"[A-Za-z_][A-Za-z0-9_]*", TokenType.IDENTIFIER), # identificadores e palavras-chave
        (r"(//|==|!=|>=|<=)", TokenType.OPERATOR),      # operadores de 2 caracteres
        (r"[+\-*/%<>]", TokenType.OPERATOR),            # operadores de 1 caractere
        (r"=", TokenType.ASSIGN),                       # atribui√ß√£o
        (r"[\(\)\[\]\{\},.:]", TokenType.DELIMITER),    # delimitadores
        (r"\r?\n", TokenType.NEWLINE),                  # quebra de linha
    ]

    def __init__(self, source: str) -> None:
        """Inicializa um novo l√©xico com o texto completo do c√≥digo-fonte."""
        self.source: str = source
        self.line: int = 1
        self.pos: int = 0
        # Estado para coment√°rios de bloco com aspas triplas
        self._in_block_comment: bool = False
        self._block_comment_delim: str | None = None  # """ ou '''

    def get_tokens(self) -> List[Token]:
        """L√™ o c√≥digo-fonte e retorna a lista de tokens terminando com EOF."""
        tokens: List[Token] = []
        indent_stack: List[int] = [0]

        # Itera linha a linha, preservando as quebras ("\n")
        lines = self.source.splitlines(keepends=True)
        for raw in lines:
            # Remove CR/LF do final para obter o conte√∫do l√≥gico
            line = raw.rstrip("\r\n")

            # Linha em branco ou apenas coment√°rio?
            stripped = line.lstrip(" \t")
            is_blank = stripped == "" or stripped.startswith("#")

            # Tratamento de coment√°rio em bloco com aspas triplas: ignora o conte√∫do
            if self._in_block_comment:
                # Encerra o bloco se o delimitador de fechamento aparecer nesta linha
                if self._block_comment_delim and self._block_comment_delim in stripped:
                    self._in_block_comment = False
                    self._block_comment_delim = None

            # Se n√£o for em branco e n√£o estiver em coment√°rio de bloco, calcula a indenta√ß√£o
            elif not is_blank:
                # Conta espa√ßos/tabs iniciais
                i = 0
                indent_count = 0
                while i < len(line) and line[i] in (" ", "\t"):
                    indent_count += 4 if line[i] == "\t" else 1
                    i += 1

                # Ajusta a pilha de indenta√ß√£o
                if indent_count > indent_stack[-1]:
                    indent_stack.append(indent_count)
                    tokens.append(Token(TokenType.INDENT, "INDENT", self.line))
                else:
                    while indent_count < indent_stack[-1]:
                        indent_stack.pop()
                        tokens.append(Token(TokenType.DEDENT, "DEDENT", self.line))
                    if indent_count != indent_stack[-1]:
                        raise LexicalError(self.line, "indenta√ß√£o inconsistente")

                # Tokeniza o restante da linha a partir de i, ou detecta coment√°rio em bloco
                segment = line[i:]
                # In√≠cio de coment√°rio em bloco com aspas triplas usado como coment√°rio/docstring
                if segment.startswith('"""') or segment.startswith("'''"):
                    delim = '"""' if segment.startswith('"""') else "'''"
                    self._in_block_comment = True
                    self._block_comment_delim = delim
                    # Se tamb√©m terminar na mesma linha, encerra imediatamente
                    if segment.count(delim) >= 2:
                        self._in_block_comment = False
                        self._block_comment_delim = None
                    # N√£o emite tokens para este conte√∫do (tratado como coment√°rio)
                else:
                    self._tokenize_segment(segment, tokens)

            # Mesmo para linhas em branco/somente coment√°rios, emite NEWLINE para o parser usar
            tokens.append(Token(TokenType.NEWLINE, "\n", self.line))
            self.line += 1

        # Fecha indenta√ß√£o restante no EOF
        while len(indent_stack) > 1:
            indent_stack.pop()
            tokens.append(Token(TokenType.DEDENT, "DEDENT", self.line))

        tokens.append(Token(TokenType.EOF, "EOF", self.line))
        return tokens

    def _tokenize_segment(self, segment: str, out_tokens: List[Token]) -> None:
        """Tokeniza um trecho de uma linha (sem caracteres de quebra de linha)."""
        pos = 0
        n = len(segment)

        while pos < n:
            match = None
            for regex, tipo in self.TOKEN_REGEX:
                if tipo == TokenType.NEWLINE:
                    continue  # n√£o separar aqui

                pattern = re.compile(regex)
                match = pattern.match(segment, pos)
                if match:
                    lexeme = match.group(0)

                    if tipo:
                        # üîπ Identificadores e palavras-chave
                        if tipo == TokenType.IDENTIFIER:
                            if lexeme in KEYWORDS:
                                out_tokens.append(Token(TokenType.KEYWORD, lexeme, self.line))
                            else:
                                if len(lexeme) > 20:
                                    raise LexicalError(
                                        self.line,
                                        "identificador com mais de 20 caracteres",
                                    )
                                out_tokens.append(Token(TokenType.IDENTIFIER, lexeme, self.line))

                        # üîπ N√∫meros
                        elif tipo == TokenType.NUMBER:
                            out_tokens.append(Token(TokenType.NUMBER, lexeme, self.line))

                        # üîπ Strings, operadores, delimitadores, atribui√ß√µes etc.
                        else:
                            out_tokens.append(Token(tipo, lexeme, self.line))

                    # Avan√ßa o ponteiro de leitura
                    pos = match.end(0)

                    # Evita tokens inv√°lidos tipo "123abc"
                    if tipo == TokenType.NUMBER and pos < n:
                        nxt = segment[pos]
                        if nxt.isalpha() or nxt == "_":
                            raise LexicalError(self.line, "identificador iniciando com n√∫mero")

                    break

            if not match:
                raise LexicalError(self.line, f"caractere inesperado '{segment[pos]}'")


__all__ = [
    "LexerPython",
]
